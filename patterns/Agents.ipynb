{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD63qFNCWxEJNQ8EuyC+BE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himsgpt/GenAI_Agentic_Patterns/blob/main/Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vBqvuBSZrRUp",
        "outputId": "a8bd7088-1d25-4f3a-9a9e-c4bdab5dd7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.59)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting llama_cpp_python\n",
            "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama_cpp_python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama_cpp_python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama_cpp_python\n",
            "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4067787 sha256=4eaf9236aa2648aecb9dcaee51a40415e495780dddb778a17bec564ce55b29e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n",
            "Successfully built llama_cpp_python\n",
            "Installing collected packages: diskcache, llama_cpp_python\n",
            "Successfully installed diskcache-5.6.3 llama_cpp_python-0.3.9\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community\n",
        "! pip install llama_cpp_python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REACT Agents"
      ],
      "metadata": {
        "id": "uZmeiAm3zPBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.agents.agent_types import AgentType"
      ],
      "metadata": {
        "id": "yvwRmIlCrkby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title define tool\n",
        "def get_today_temperature(input : str) -> str:\n",
        "  return (\"temperature is 45 degree\")\n",
        "\n",
        "\n",
        "def get_today_date(input : str) -> str:\n",
        "    import datetime\n",
        "    today = datetime.date.today()\n",
        "    return f\"\\n {today} \\n\"\n",
        "\n",
        "# Define the Tools\n",
        "\n",
        "get_today_temperature_tool = Tool(\n",
        "    name=\"Get today's temperature\",\n",
        "    func=get_today_temperature,\n",
        "    description=\"Useful for getting age information for any person. Input should be the name of the person.\"\n",
        ")\n",
        "get_today_date_tool = Tool(\n",
        "    name=\"Get toda's date\",\n",
        "    func=get_today_date,\n",
        "    description=\"Useful for getting age information for any person. Input should be the name of the person.\"\n",
        ")\n",
        "\n",
        "tools = [get_today_temperature_tool,get_today_date_tool ]"
      ],
      "metadata": {
        "id": "sdOcERNgs0zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pull prompt react\n",
        "from langchain import hub\n",
        "prompt_react = hub.pull(\"hwchase17/react\")\n",
        "print(prompt_react.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8zml70tR-o",
        "outputId": "b8d29658-fd15-446b-a937-7ac4f72934cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title llms"
      ],
      "metadata": {
        "id": "kxDdnXndvE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "!wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"phi-2.Q4_K_M.gguf\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=200,\n",
        "    n_ctx=2048,\n",
        "    n_threads=4,\n",
        "    n_gpu_layers=0  # CPU-only\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F0pV1rSZtj3A",
        "outputId": "c4172e19-5ad6-496f-f797-63292f4ab1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-19 08:56:02--  https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.44, 3.166.152.65, 3.166.152.110, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6580aa20419afba19a692cc8/cb5d304e5b36d2f91430fff1530842167680b0958c4083b09e04d4dbf8cf7a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250519T085602Z&X-Amz-Expires=3600&X-Amz-Signature=873f18e00908ac45f42af7cd04a6eed43d407de37137a5907a95e3c992a42148&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-2.Q4_K_M.gguf%3B+filename%3D%22phi-2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1747648562&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzY0ODU2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTgwYWEyMDQxOWFmYmExOWE2OTJjYzgvY2I1ZDMwNGU1YjM2ZDJmOTE0MzBmZmYxNTMwODQyMTY3NjgwYjA5NThjNDA4M2IwOWUwNGQ0ZGJmOGNmN2EwOCoifV19&Signature=uZJsdq2Z8mgcdQNLjwltvj9IhL7ZcO8GWApXqj9csOtQ9p4yblQHCTIO3PU9WLuRv2iJI7VbDnTrGmvwTNB1eaBh4KAguTnsjwie3v-vz5DxhnA6GFNvJ7J1IPyZFbYQSbDH6mrw1W0vPCUsjz2cajV58cMbm2Bf5FCzDC4p8JVYF3pB0APguyqKv66iebxw3%7E2wX7tADIJWLsu2IPiAXf6I0UFtKhKG9CJloLR%7EmKkVkVC9hL2lUFVFfiO6Xw3zyBu3rcz0fNdkXCSvVR%7ECpelZOPHjrSLn6yLpDJ8L6RpP7o1W1sGh7APEhcv1AJDWok3OnyKRWE18hNC5Gw0c6Q__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-05-19 08:56:02--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6580aa20419afba19a692cc8/cb5d304e5b36d2f91430fff1530842167680b0958c4083b09e04d4dbf8cf7a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250519T085602Z&X-Amz-Expires=3600&X-Amz-Signature=873f18e00908ac45f42af7cd04a6eed43d407de37137a5907a95e3c992a42148&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-2.Q4_K_M.gguf%3B+filename%3D%22phi-2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1747648562&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzY0ODU2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTgwYWEyMDQxOWFmYmExOWE2OTJjYzgvY2I1ZDMwNGU1YjM2ZDJmOTE0MzBmZmYxNTMwODQyMTY3NjgwYjA5NThjNDA4M2IwOWUwNGQ0ZGJmOGNmN2EwOCoifV19&Signature=uZJsdq2Z8mgcdQNLjwltvj9IhL7ZcO8GWApXqj9csOtQ9p4yblQHCTIO3PU9WLuRv2iJI7VbDnTrGmvwTNB1eaBh4KAguTnsjwie3v-vz5DxhnA6GFNvJ7J1IPyZFbYQSbDH6mrw1W0vPCUsjz2cajV58cMbm2Bf5FCzDC4p8JVYF3pB0APguyqKv66iebxw3%7E2wX7tADIJWLsu2IPiAXf6I0UFtKhKG9CJloLR%7EmKkVkVC9hL2lUFVFfiO6Xw3zyBu3rcz0fNdkXCSvVR%7ECpelZOPHjrSLn6yLpDJ8L6RpP7o1W1sGh7APEhcv1AJDWok3OnyKRWE18hNC5Gw0c6Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.173.166.82, 18.173.166.85, 18.173.166.5, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.173.166.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1789239136 (1.7G)\n",
            "Saving to: ‘phi-2.Q4_K_M.gguf’\n",
            "\n",
            "phi-2.Q4_K_M.gguf   100%[===================>]   1.67G   151MB/s    in 14s     \n",
            "\n",
            "2025-05-19 08:56:16 (122 MB/s) - ‘phi-2.Q4_K_M.gguf’ saved [1789239136/1789239136]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from phi-2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
            "load: missing pre-tokenizer type, using: 'default'\n",
            "load:                                             \n",
            "load: ************************************        \n",
            "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "load: CONSIDER REGENERATING THE MODEL             \n",
            "load: ************************************        \n",
            "load:                                             \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 944\n",
            "load: token to piece cache size = 0.3151 MB\n",
            "print_info: arch             = phi2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 2560\n",
            "print_info: n_embd_v_gqa     = 2560\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 2.78 B\n",
            "print_info: general.name     = Phi2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 51200\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =   787.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "..............................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 64\n",
            "llama_context: n_ubatch      = 8\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.20 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
            "llama_kv_cache_unified: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
            "llama_context:        CPU compute buffer size =     3.05 MiB\n",
            "llama_context: graph nodes  = 1289\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize ReAct agent\n",
        "# option 1\n",
        "# from langchain.agents import initialize_agent\n",
        "# agent = initialize_agent(\n",
        "#     llm = llm,\n",
        "#     tools = tools,\n",
        "#     agent=prompt_react.template,\n",
        "#     verbose = True\n",
        "# )\n",
        "# agent.run(\"What is today's date?\")\n",
        "\n",
        "# option 2\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "react_agent = create_react_agent(llm, tools=tools, prompt=prompt_react)\n",
        "react_agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=react_agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")\n",
        "react_agent_executor.invoke({\"input\": \"What is today's date?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YHFrvyGUvSYd",
        "outputId": "264ff316-0251-4291-949d-9b4593caed48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 20 prefix-match hit, remaining 201 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =   39014.44 ms /   201 tokens (  194.10 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12131.90 ms /    38 runs   (  319.26 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   51219.97 ms /   239 tokens\n",
            "Llama.generate: 226 prefix-match hit, remaining 56 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to get today's date\n",
            "Action: Get toda' s date(input: str) -> str  - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =   11086.64 ms /    56 tokens (  197.98 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9524.05 ms /    30 runs   (  317.47 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =   20665.07 ms /    86 tokens\n",
            "Llama.generate: 298 prefix-match hit, remaining 36 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    6106.55 ms /    36 tokens (  169.63 ms per token,     5.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9832.43 ms /    31 runs   (  317.18 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =   15994.95 ms /    67 tokens\n",
            "Llama.generate: 365 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3649.03 ms /    21 tokens (  173.76 ms per token,     5.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9928.70 ms /    31 runs   (  320.28 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =   13632.32 ms /    52 tokens\n",
            "Llama.generate: 417 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    5031.74 ms /    21 tokens (  239.61 ms per token,     4.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9150.75 ms /    31 runs   (  295.19 ms per token,     3.39 tokens per second)\n",
            "llama_perf_context_print:       total time =   14235.32 ms /    52 tokens\n",
            "Llama.generate: 469 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    5430.84 ms /    21 tokens (  258.61 ms per token,     3.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9223.59 ms /    31 runs   (  297.54 ms per token,     3.36 tokens per second)\n",
            "llama_perf_context_print:       total time =   14709.16 ms /    52 tokens\n",
            "Llama.generate: 521 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    5149.85 ms /    21 tokens (  245.23 ms per token,     4.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10680.18 ms /    31 runs   (  344.52 ms per token,     2.90 tokens per second)\n",
            "llama_perf_context_print:       total time =   15886.36 ms /    52 tokens\n",
            "Llama.generate: 573 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3879.26 ms /    21 tokens (  184.73 ms per token,     5.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10421.78 ms /    31 runs   (  336.19 ms per token,     2.97 tokens per second)\n",
            "llama_perf_context_print:       total time =   14357.81 ms /    52 tokens\n",
            "Llama.generate: 625 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3729.88 ms /    21 tokens (  177.61 ms per token,     5.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10544.57 ms /    31 runs   (  340.15 ms per token,     2.94 tokens per second)\n",
            "llama_perf_context_print:       total time =   14330.29 ms /    52 tokens\n",
            "Llama.generate: 677 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3743.07 ms /    21 tokens (  178.24 ms per token,     5.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10701.21 ms /    31 runs   (  345.20 ms per token,     2.90 tokens per second)\n",
            "llama_perf_context_print:       total time =   14500.16 ms /    52 tokens\n",
            "Llama.generate: 729 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3819.66 ms /    21 tokens (  181.89 ms per token,     5.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10825.80 ms /    31 runs   (  349.22 ms per token,     2.86 tokens per second)\n",
            "llama_perf_context_print:       total time =   14700.79 ms /    52 tokens\n",
            "Llama.generate: 781 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    5563.26 ms /    21 tokens (  264.92 ms per token,     3.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9906.98 ms /    31 runs   (  319.58 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   15523.76 ms /    52 tokens\n",
            "Llama.generate: 833 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    5422.64 ms /    21 tokens (  258.22 ms per token,     3.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11003.55 ms /    31 runs   (  354.95 ms per token,     2.82 tokens per second)\n",
            "llama_perf_context_print:       total time =   16481.45 ms /    52 tokens\n",
            "Llama.generate: 885 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3900.63 ms /    21 tokens (  185.74 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11253.01 ms /    31 runs   (  363.00 ms per token,     2.75 tokens per second)\n",
            "llama_perf_context_print:       total time =   15208.53 ms /    52 tokens\n",
            "Llama.generate: 937 prefix-match hit, remaining 21 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   43225.61 ms\n",
            "llama_perf_context_print: prompt eval time =    3907.00 ms /    21 tokens (  186.05 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11374.54 ms /    31 runs   (  366.92 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =   15338.23 ms /    52 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Action: Get today' s temperature(input: str) -> str - This is the function you will use for this question.\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"What is today's date?\",\n",
              " 'output': 'Agent stopped due to iteration limit or time limit.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection Agent"
      ],
      "metadata": {
        "id": "kVr4yU1V28zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.reflexion.base import ReflexionAgent\n",
        "from langchain.agents.reflexion.prompt import REFLEXION_FEEDBACK_PROMPT\n",
        "\n",
        "reflection_agent = ReflexionAgent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    max_iterations=3,  # tries, with reflection after failure\n",
        "    max_reflections=1,\n",
        "    feedback_prompt=REFLEXION_FEEDBACK_PROMPT,\n",
        ")\n",
        "result = reflection_agent.run(\"What is today's date?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "SdsBnviT27Py",
        "outputId": "3d977671-28b3-4b47-bbf6-3eecaa92dd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.agents.reflexion'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f885a9987439>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreflexion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReflexionAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreflexion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mREFLEXION_FEEDBACK_PROMPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m reflection_agent = ReflexionAgent(\n\u001b[1;32m      5\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.agents.reflexion'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}
