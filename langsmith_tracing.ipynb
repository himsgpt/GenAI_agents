{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL2afie5JM9FUJaATvTzso",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himsgpt/GenAI_agents/blob/main/langsmith_tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h_wziA3YmOq3",
        "outputId": "5c6617a8-0375-49e2-cd94-108dae0a186e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.12/dist-packages (from langchain_experimental) (0.3.75)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.20)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.9)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.2.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, groq, dataclasses-json, langchain-community, langchain_experimental\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.31.0 langchain-community-0.3.29 langchain_experimental-0.3.4 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U langchain-core langchain-openai langgraph langsmith requests\n",
        "!pip install langchain_experimental groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langsmith\n",
        "import os\n",
        "from langsmith import trace\n",
        "from groq import Groq\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Configure API keys - replace with your actual keys\n",
        "# os.environ['OPENAI_API_KEY'] = ''\n",
        "os.environ['LANGSMITH_API_KEY'] = 'xx'\n",
        "os.environ['LANGSMITH_PROJECT'] = 'langsmith-tutorial-demo' #This is the project name where the traces will be stored\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "\n",
        "# Verify configuration\n",
        "required_vars = ['LANGCHAIN_API_KEY']\n",
        "for var in required_vars:\n",
        "    if not os.getenv(var) or 'your_' in os.getenv(var, ''):\n",
        "        print(f\"Warning: {var} needs your actual key\")\n",
        "    else:\n",
        "        print(f\"✓ {var} configured\")\n",
        "\n",
        "print(f\"\\nLangSmith Project: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
        "print(\"\\nTracing is now active - all AI operations will be logged for analysis\")\n",
        "print(\"Visit https://smith.langchain.com to see your traces\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocITNgNqmq6p",
        "outputId": "ac5f2f0f-b5a0-4f5d-cb47-9427bacd7fa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LANGCHAIN_API_KEY needs your actual key\n",
            "\n",
            "LangSmith Project: None\n",
            "\n",
            "Tracing is now active - all AI operations will be logged for analysis\n",
            "Visit https://smith.langchain.com to see your traces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_client = Groq(api_key=\"xx\")\n",
        "\n",
        "def call_groq(messages):\n",
        "    groq_messages = [{\"role\": \"user\", \"content\": msg.content} for msg in messages]\n",
        "\n",
        "    with trace(\"groq_call\", inputs={\"messages\": groq_messages}):\n",
        "        resp = groq_client.chat.completions.create(\n",
        "            messages=groq_messages,\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "        )\n",
        "\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "result = call_groq([HumanMessage(content=\"Give me best LLM today?\")])\n",
        "print(\"Groq says:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTVpVIFCvm6r",
        "outputId": "448b9941-897c-4e18-e6e9-4deba8890bbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq says: As of my knowledge cutoff in December 2023, some of the best Large Language Models (LLMs) include:\n",
            "\n",
            "1. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI, LLaMA is a state-of-the-art LLM that has achieved excellent results in various natural language processing (NLP) tasks.\n",
            "2. **PaLM (Pathways Language Model)**: Released by Google, PaLM is a highly advanced LLM that has demonstrated exceptional performance in tasks like language translation, question-answering, and text summarization.\n",
            "3. **BLOOM (BigLanguage Open Oncology Model)**: BLOOM is an open-source LLM developed by a consortium of researchers, which has shown impressive results in tasks like language translation, sentiment analysis, and text classification.\n",
            "4. **LLaMA-2**: An updated version of the LLaMA model, LLaMA-2 has achieved even better results in various NLP tasks, solidifying its position as one of the top LLMs.\n",
            "5. **GPT-4 (Generative Pre-trained Transformer 4)**: Developed by OpenAI, GPT-4 is a highly advanced LLM that has demonstrated exceptional performance in tasks like language translation, text summarization, and conversation generation.\n",
            "\n",
            "Please note that the field of LLMs is rapidly evolving, and new models are being developed and released regularly. The ranking of the \"best\" LLM can vary depending on the specific task, dataset, or application.\n",
            "\n",
            "Would you like me to provide more information about any of these models or help with a specific task?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"Simple state that flows through our agent workflow.\"\"\"\n",
        "    user_question: str        # The original question from the user\n",
        "    needs_search: bool        # Whether we determined search is needed\n",
        "    search_result: str        # Result from our search tool (if used)\n",
        "    final_answer: str         # The response we'll give to the user\n",
        "    reasoning: str            # Why we made our decisions (great for observability)\n",
        "\n",
        "print(\"Agent state schema defined\")\n",
        "print(\"This structured state enables LangSmith to track information flow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJintqiSxh2z",
        "outputId": "ae413922-8842-4de0-e283-9cd36f101f86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent state schema defined\n",
            "This structured state enables LangSmith to track information flow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def wikipedia_search(query: str) -> str:\n",
        "    \"\"\"Search Wikipedia for current information about a topic.\"\"\"\n",
        "    try:\n",
        "        # Use Wikipedia's proper search API that can handle general queries\n",
        "        # This is different from the page summary API which requires exact page titles\n",
        "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "        search_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": query,\n",
        "            \"format\": \"json\",\n",
        "            \"srlimit\": 3  # Get top 3 results\n",
        "        }\n",
        "\n",
        "        response = requests.get(search_url, params=search_params, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            search_results = data.get('query', {}).get('search', [])\n",
        "\n",
        "            if search_results:\n",
        "                # Get the most relevant result and fetch its summary\n",
        "                top_result = search_results[0]\n",
        "                page_title = top_result['title']\n",
        "\n",
        "                # Now get the page summary using the exact title\n",
        "                summary_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
        "                summary_response = requests.get(summary_url, timeout=10)\n",
        "\n",
        "                if summary_response.status_code == 200:\n",
        "                    summary_data = summary_response.json()\n",
        "                    extract = summary_data.get('extract', 'No summary available')\n",
        "                    # Truncate for readability in traces\n",
        "                    return f\"Found information about '{page_title}': {extract[:400]}...\"\n",
        "                else:\n",
        "                    return f\"Found '{page_title}' but couldn't retrieve summary\"\n",
        "            else:\n",
        "                return f\"No Wikipedia articles found for '{query}'\"\n",
        "        else:\n",
        "            return f\"Wikipedia search failed with status {response.status_code}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # This error handling will show up in LangSmith traces\n",
        "        return f\"Search error: {str(e)}\"\n",
        "\n",
        "print(\"Search tool created with proper Wikipedia search API integration\")\n",
        "print(\"Tool execution timing and results will be captured automatically\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_24mTvjvxo1H",
        "outputId": "55628c43-dd9c-4bac-dc0a-b0580cba8b0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search tool created with proper Wikipedia search API integration\n",
            "Tool execution timing and results will be captured automatically\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Dict, Any\n",
        "from groq import Groq\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langchain_core.tools import BaseTool\n",
        "\n",
        "# Initialize Groq client\n",
        "# done\n",
        "\n",
        "class GroqChat(BaseChatModel):\n",
        "    model: str = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq-chat\"\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        # Convert LangChain messages to Groq format\n",
        "        groq_messages = []\n",
        "        for msg in messages:\n",
        "            if isinstance(msg, SystemMessage):\n",
        "                role = \"system\"\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                role = \"assistant\"\n",
        "            else:\n",
        "                role = \"user\"\n",
        "            groq_messages.append({\"role\": role, \"content\": msg.content})\n",
        "\n",
        "        # Call Groq API\n",
        "        response = groq_client.chat.completions.create(\n",
        "            messages=groq_messages,\n",
        "            model=self.model,\n",
        "            stop=stop,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Extract the response content\n",
        "        content = response.choices[0].message.content\n",
        "\n",
        "        # Create the generation object\n",
        "        generation = ChatGeneration(message=AIMessage(content=content))\n",
        "\n",
        "        # Return a ChatResult (not LLMResult)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    def bind_tools(self, tools: List[BaseTool], **kwargs):\n",
        "        \"\"\"Bind tools to the model for tool calling.\"\"\"\n",
        "        # For Groq, we'll need to handle tool calling manually\n",
        "        # since it doesn't natively support function calling\n",
        "        return self\n",
        "\n",
        "# Create the model instance\n",
        "groq_model = GroqChat()"
      ],
      "metadata": {
        "id": "7eJV0O-Ox_FE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_search_need(state: AgentState) -> AgentState:\n",
        "    \"\"\"Analyze the question and decide if we need to search for current information.\"\"\"\n",
        "    user_question = state[\"user_question\"]\n",
        "\n",
        "    # This prompt engineering is visible in LangSmith traces\n",
        "    # Notice how we're asking for a structured response to make parsing reliable\n",
        "    decision_prompt = f\"\"\"\n",
        "    Analyze this question and decide if it requires current/recent information that might not be in your training data:\n",
        "\n",
        "    Question: \"{user_question}\"\n",
        "\n",
        "    Consider:\n",
        "    - Does this ask about recent events, current prices, or breaking news?\n",
        "    - Does this ask about people, companies, or topics that change frequently?\n",
        "    - Can you answer this well using your existing knowledge?\n",
        "\n",
        "    Respond with exactly \"SEARCH\" if you need current information, or \"DIRECT\" if you can answer directly.\n",
        "    Then on a new line, briefly explain your reasoning.\n",
        "    \"\"\"\n",
        "\n",
        "    response = groq_model.invoke([SystemMessage(content=decision_prompt)])\n",
        "    decision_text = response.content.strip()\n",
        "\n",
        "    # Parse the response - this parsing logic will be visible in traces\n",
        "    lines = decision_text.split('\\n')\n",
        "    decision = lines[0].strip()\n",
        "    reasoning = lines[1] if len(lines) > 1 else \"No reasoning provided\"\n",
        "\n",
        "    # Update state with our decision\n",
        "    state[\"needs_search\"] = decision == \"SEARCH\"\n",
        "    state[\"reasoning\"] = f\"Decision: {decision}. Reasoning: {reasoning}\"\n",
        "\n",
        "    # This print statement will help you see the flow during execution\n",
        "    print(f\"Decision: {'SEARCH' if state['needs_search'] else 'DIRECT'} - {reasoning}\")\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "A1zivRdPx5gT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_search(state: AgentState) -> AgentState:\n",
        "    \"\"\"Execute search if needed, otherwise skip this step.\"\"\"\n",
        "    if not state[\"needs_search\"]:\n",
        "        print(\"Skipping search - not needed for this question\")\n",
        "        state[\"search_result\"] = \"No search performed\"\n",
        "        return state\n",
        "\n",
        "    print(f\"Executing search for: {state['user_question']}\")\n",
        "\n",
        "    # Execute our search tool - this will show up as a separate step in LangSmith\n",
        "    # The .invoke() call will be traced with full input/output details\n",
        "    search_result = wikipedia_search.invoke({\"query\": state[\"user_question\"]})\n",
        "    state[\"search_result\"] = search_result\n",
        "\n",
        "    print(f\"Search completed: {len(search_result)} characters returned\")\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "UUN1Zd4zyGYK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    \"\"\"Generate the final response using all available information.\"\"\"\n",
        "    user_question = state[\"user_question\"]\n",
        "    search_result = state.get(\"search_result\", \"\")\n",
        "    used_search = state[\"needs_search\"]\n",
        "\n",
        "    # Build context for the response\n",
        "    # This conditional logic creates different prompt patterns that LangSmith will capture\n",
        "    if used_search and \"Search error\" not in search_result:\n",
        "        context = f\"Question: {user_question}\\n\\nSearch Results: {search_result}\"\n",
        "        response_prompt = f\"\"\"\n",
        "        Answer the user's question using both your knowledge and the search results provided.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Provide a helpful, accurate response that synthesizes the information.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        response_prompt = f\"\"\"\n",
        "        Answer this question using your existing knowledge:\n",
        "\n",
        "        {user_question}\n",
        "\n",
        "        Provide a helpful, accurate response.\n",
        "        \"\"\"\n",
        "\n",
        "    # This LLM call will be traced with the complete prompt and response\n",
        "    response = groq_model.invoke([SystemMessage(content=response_prompt)])\n",
        "    state[\"final_answer\"] = response.content\n",
        "\n",
        "    print(f\"Response generated: {len(response.content)} characters\")\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "dDOeRvbKyOOz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the workflow graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add our three workflow steps\n",
        "# Each of these will appear as a distinct node in LangSmith's graph visualization\n",
        "workflow.add_node(\"decide\", decide_search_need)\n",
        "workflow.add_node(\"search\", execute_search)\n",
        "workflow.add_node(\"respond\", generate_response)\n",
        "\n",
        "# Define the flow with conditional logic\n",
        "# LangSmith will show you which edges were traversed for each execution\n",
        "workflow.set_entry_point(\"decide\")\n",
        "workflow.add_edge(\"decide\", \"search\")     # Always go to search step (it will skip if not needed)\n",
        "workflow.add_edge(\"search\", \"respond\")    # Then generate response\n",
        "workflow.add_edge(\"respond\", END)         # Finish\n",
        "\n",
        "# Compile into an executable agent\n",
        "simple_agent = workflow.compile()\n",
        "\n",
        "print(\"Workflow compiled successfully\")\n",
        "print(\"Flow: decide → search → generate_response\")\n",
        "print(\"Ready to demonstrate LangSmith observability\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIPrmMbvyQ51",
        "outputId": "3b74054a-9a5d-4936-a9b6-eaafec67909c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workflow compiled successfully\n",
            "Flow: decide → search → generate_response\n",
            "Ready to demonstrate LangSmith observability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install graphviz pymermaid\n",
        "!pip install pyppeteer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "collapsed": true,
        "id": "-3McgHUz006a",
        "outputId": "41241579-d77a-4f13-e1ae-bd185da15eb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.12/dist-packages (from pyppeteer) (2025.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from pyppeteer) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.14.1)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: websockets\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107329 sha256=bc7dd67d20f956c3418764a93f75afd1bea51f23a54e1a389d274fc4adad9493\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built websockets\n",
            "Installing collected packages: appdirs, websockets, urllib3, pyee, pyppeteer\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.35.0 requires urllib3[socks]<3.0,>=2.5.0, but you have urllib3 1.26.20 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.31.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "google-adk 1.12.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 pyee-11.1.1 pyppeteer-2.0.0 urllib3-1.26.20 websockets-10.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "e27697b0e4b14789a3421e8fa834b4ea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Image, display\n",
        "\n",
        "# display(\n",
        "#     Image(\n",
        "#         simple_agent.get_graph().draw_mermaid_png(\n",
        "#             max_retries=5,\n",
        "#             retry_delay=2.0\n",
        "#         )\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "tOq8huUY0mry"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.runnables.graph_mermaid import MermaidDrawMethod\n",
        "# from IPython.display import Image, display\n",
        "\n",
        "# display(\n",
        "#     Image(\n",
        "#         simple_agent.get_graph().draw_mermaid_png(\n",
        "#             draw_method=MermaidDrawMethod.PYPPETEER\n",
        "#         )\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "Qt5Sw_7Y1x0j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test_with_observability(question: str, test_type: str) -> dict:\n",
        "    \"\"\"Run a test and capture comprehensive observability data.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {question}\")\n",
        "    print(f\"Type: {test_type}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize state for this test\n",
        "    initial_state = {\n",
        "        \"user_question\": question,\n",
        "        \"needs_search\": False,\n",
        "        \"search_result\": \"\",\n",
        "        \"final_answer\": \"\",\n",
        "        \"reasoning\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Execute with metadata for LangSmith\n",
        "        # This metadata will help you filter and analyze traces later\n",
        "        config = {\n",
        "            \"metadata\": {\n",
        "                \"test_type\": test_type,\n",
        "                \"tutorial\": \"langsmith-observability\"\n",
        "            },\n",
        "            \"tags\": [\"tutorial\", \"demo\", test_type]\n",
        "        }\n",
        "\n",
        "        # This invoke call will create a complete trace in LangSmith\n",
        "        final_state = simple_agent.invoke(initial_state, config=config)\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time = end_time - start_time\n",
        "\n",
        "        # Display results for immediate feedback\n",
        "        print(f\"\\nResults:\")\n",
        "        print(f\"   Decision Process: {final_state['reasoning']}\")\n",
        "        print(f\"   Used Search: {'Yes' if final_state['needs_search'] else 'No'}\")\n",
        "        print(f\"   Response Length: {len(final_state['final_answer'])} characters\")\n",
        "        print(f\"   Total Time: {total_time:.2f} seconds\")\n",
        "        print(f\"\\nAnswer: {final_state['final_answer'][:200]}...\")\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"type\": test_type,\n",
        "            \"success\": True,\n",
        "            \"used_search\": final_state['needs_search'],\n",
        "            \"total_time\": round(total_time, 2),\n",
        "            \"reasoning\": final_state['reasoning']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"type\": test_type,\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }"
      ],
      "metadata": {
        "id": "vSjWQfv_yyvL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"type\": \"direct_answer\",\n",
        "        \"expected_search\": False\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What happened in the 2024 US presidential election?\",\n",
        "        \"type\": \"current_info\",\n",
        "        \"expected_search\": True\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Tell me about artificial intelligence\",\n",
        "        \"type\": \"factual_lookup\",\n",
        "        \"expected_search\": False  # Should be answerable directly\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Starting LangSmith Observability Demo\")\n",
        "print(\"Each test will generate detailed traces in your LangSmith dashboard\")\n",
        "print(\"Visit https://smith.langchain.com to see real-time traces\\n\")\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\nRunning Test {i}/{len(test_cases)}\")\n",
        "\n",
        "    result = run_test_with_observability(\n",
        "        test_case[\"question\"],\n",
        "        test_case[\"type\"]\n",
        "    )\n",
        "\n",
        "    test_results.append(result)\n",
        "\n",
        "    # Small delay to make the traces easier to distinguish in LangSmith\n",
        "    time.sleep(1)\n",
        "\n",
        "print(f\"\\n\\nAll tests completed\")\n",
        "print(f\"Generated {len(test_results)} traces in LangSmith\")\n",
        "print(f\"Check your dashboard to explore the detailed execution data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-pv8I0Qy7_w",
        "outputId": "660b0035-bf74-4fa6-b9c6-d6a2c4600da3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LangSmith Observability Demo\n",
            "Each test will generate detailed traces in your LangSmith dashboard\n",
            "Visit https://smith.langchain.com to see real-time traces\n",
            "\n",
            "\n",
            "Running Test 1/3\n",
            "\n",
            "============================================================\n",
            "Testing: What is the capital of France?\n",
            "Type: direct_answer\n",
            "============================================================\n",
            "Decision: DIRECT - The question asks for the capital of France, which is a historical and geographical fact that does not change frequently, allowing for a direct answer based on existing knowledge.\n",
            "Skipping search - not needed for this question\n",
            "Response generated: 31 characters\n",
            "\n",
            "Results:\n",
            "   Decision Process: Decision: DIRECT. Reasoning: The question asks for the capital of France, which is a historical and geographical fact that does not change frequently, allowing for a direct answer based on existing knowledge.\n",
            "   Used Search: No\n",
            "   Response Length: 31 characters\n",
            "   Total Time: 0.81 seconds\n",
            "\n",
            "Answer: The capital of France is Paris....\n",
            "\n",
            "Running Test 2/3\n",
            "\n",
            "============================================================\n",
            "Testing: What happened in the 2024 US presidential election?\n",
            "Type: current_info\n",
            "============================================================\n",
            "Decision: SEARCH - The question asks about the 2024 US presidential election, which is a future event that has not yet occurred and therefore requires current or recent information that may not be available in the training data.\n",
            "Executing search for: What happened in the 2024 US presidential election?\n",
            "Search completed: 39 characters returned\n",
            "Response generated: 1082 characters\n",
            "\n",
            "Results:\n",
            "   Decision Process: Decision: SEARCH. Reasoning: The question asks about the 2024 US presidential election, which is a future event that has not yet occurred and therefore requires current or recent information that may not be available in the training data.\n",
            "   Used Search: Yes\n",
            "   Response Length: 1082 characters\n",
            "   Total Time: 1.32 seconds\n",
            "\n",
            "Answer: I'm sorry, but I'm unable to provide information on the 2024 US presidential election as my knowledge cutoff is December 2023, and I do not have real-time access to current events. Additionally, the s...\n",
            "\n",
            "Running Test 3/3\n",
            "\n",
            "============================================================\n",
            "Testing: Tell me about artificial intelligence\n",
            "Type: factual_lookup\n",
            "============================================================\n",
            "Decision: DIRECT - This question is a general inquiry about artificial intelligence, which is a broad and well-established field. The fundamentals of AI are well-documented and unlikely to change rapidly, allowing for a comprehensive answer using existing knowledge.\n",
            "Skipping search - not needed for this question\n",
            "Response generated: 3610 characters\n",
            "\n",
            "Results:\n",
            "   Decision Process: Decision: DIRECT. Reasoning: This question is a general inquiry about artificial intelligence, which is a broad and well-established field. The fundamentals of AI are well-documented and unlikely to change rapidly, allowing for a comprehensive answer using existing knowledge.\n",
            "   Used Search: No\n",
            "   Response Length: 3610 characters\n",
            "   Total Time: 2.12 seconds\n",
            "\n",
            "Answer: Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as:\n",
            "\n",
            "1. **Learning**: AI systems can learn from ...\n",
            "\n",
            "\n",
            "All tests completed\n",
            "Generated 3 traces in LangSmith\n",
            "Check your dashboard to explore the detailed execution data\n"
          ]
        }
      ]
    }
  ]
}